{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayes Models\n",
    "\n",
    "### Data Science 410\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far, we have mostly work with frequentist statistical methods. Frequentist models make inferences using only data and model assumptions. But, there is another class of statistical models with a long and successful history, Bayesian models. In contrast to the frequentist approach, Bayesian models use **prior information** as well as data and model assumptions to perform inferences.   \n",
    "\n",
    "Despite the long history, Bayesian models have not been used extensively until recently. This limited use is a result of many reasons. The need to specify the prior information in the form of a **prior distribution** has proved a formidable intellectual obstacle and is often cited as a reason for not using Bayesian methods. Further, modern Bayesian methods are often computationally intensive and have become practical only in the past few decades.        \n",
    "\n",
    "<img src=\"img/Sun.png\" alt=\"Drawing\" style=\"width:350px; height:450px\"/>\n",
    "<center>A Bayesian would win this bet</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief history\n",
    "\n",
    "A restricted version of Bayes Theorem was proposed by Rev.Thomas Bayes (1702-1761). Bayes Theorem, was published posthumously by his friend Richard Price. Bayes' interest was in probabilities of gambling games. He was also a supporter of Isac Newton's new theory of calculus, with his publication, *An Introduction to the Doctrine of Fluxions, and a Defence of the Mathematicians Against the Objections of the Author of The Analyst*.\n",
    "\n",
    "![](img/ThomasBayes.gif)\n",
    "\n",
    "Pierre-Simon Laplace published a version of Bayes Theorem, similar to its modern form, in his Essai philosophique sur les probabilit√©s 1814. Laplace applied Bayesian methods to problems in celestial mechanics. These problems had great practical importance in the late 18th and early 19th centuries for the safe navigation of ships. \n",
    " \n",
    "![](img/Laplace.jpg)\n",
    "\n",
    " ### Early 20th Century History\n",
    "\n",
    "The geophysicist and mathematician Harold Jefferys extensively used Bayes' methods. His 1939 book, *The Theory of Probability* was in deliberate opposition to Fisher's methods using p-values. The publication of this book set off a feud between Jefferys and Fisher which lasted until the death of both men in the 1970s. The feud resulted in Bayesian methods rarely being taught or used in scientific publications. Thus, the development and use of Bayesian was limited for a long period of time.   \n",
    "\n",
    "<img src=\"img/JeffreysProbability.jpg\" alt=\"Drawing\" style=\"width:225px; height:300px\"/>\n",
    "<center>Jefferys' seminal 1939 book</center>\n",
    "\n",
    "Despite the philosophical squabbles, Bayesian methods endured and showed an increasing number of success stories. Pragmatists continued to use both approaches. A number of success during the Second World War, with the philosophical battles raging, included:\n",
    "\n",
    "- Bayesian models were used to improve artillery accuracy in both world wars. In particular the Soviet statistician Kolmagorov used Bayesian methods to greatly improve artillery accuracy. \n",
    "- Bayesian models were used by Alan Turing to break German codes.\n",
    "- Bernard Koopman, working for the British Royal Navy, improved the ability to locate U-boats using directional data from intercepted radio transmissions. \n",
    "\n",
    "### Late 20th Century History  \n",
    "\n",
    "Starting in the second half of the 20th century the convergence of greater computing power and general acceptance lead to the following notable advances in computational Bayesian methods. The following publications are notable milestones in the advancement of Bayesian methods:\n",
    "\n",
    "- Statistical sampling using Monte Carlo methods; Stanislaw Ulam, John von Neuman; 1946, 1947\n",
    "- MCMC, or Markov Chain Monte Carlo; Metropolis et al. (1953) Journal of Chemical Physics\n",
    "- Monte Carlo sampling methods using Markov chains and their application, Hastings (1970)\n",
    "- Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images, Geman and Geman (1984) \n",
    "- Hamiltonian MCMC, Duane, Kennedy, Pendleton, and Roweth (1987)\n",
    "- Sampling-based approaches to calculating marginal densities, Gelfand and Smith (1990)\n",
    "\n",
    "### 21st Century  \n",
    "\n",
    "In the 21st Century Bayesian models are in daily routine use. These applications range across the scope of statistical model applications. A few examples include medical research, natural language understanding, and web search.  One of the more interesting applications is in management of [search and rescue operations](https://sinews.siam.org/Details-Page/bayesian-search-for-missing-aircraft-ships-and-people). Bayesian models have found uses in [legal judgements](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4934658/) when faced with diverse and uncertain evidence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Baysian vs. Frequentist Views\n",
    "\n",
    "The battle between Fisher, Jefferys and their prot√©g√©s continued for most of the 20th century. This battle was bitter and often personal. The core of these argument were:\n",
    "\n",
    "- Fisher argued that the selection of a Bayesian prior distribution was purely subjective, allowing one to achieve any answer desired.\n",
    "- Jefferys argued that all knowledge is in fact subjective, and that choosing a cut-off value or confidence interval was subjective in any event.\n",
    "\n",
    "With greater computational power and general acceptance, Bayes methods are now widely used. Among pragmatists, the common belief today is that some problems are better handled by frequentist methods and some with Bayesian methods. Models that fall between these extremes are also in common use.  \n",
    "\n",
    "Let's summarize the differences between the Bayesian and frequentist views. \n",
    "\n",
    "- The goal of Bayesian methods computation of a **posterior distribution**. Bayesian methods use **prior distributions** combined with **evidence** to compute the posterior distribution. \n",
    "- Frequentists do not quantify anything about the parameters, using p-values and confidence intervals to express the properties of parameters given the data.\n",
    "\n",
    "Recalling that both views are useful, we can contrast these methods with a chart.\n",
    "\n",
    "<img src=\"img/FrequentistBayes.jpg\" alt=\"Drawing\" style=\"width:500px; height:325px\"/>\n",
    "<center>Comparison of Bayesian and frequentist models</center>\n",
    "\n",
    "\n",
    "![](img/.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "There are numerous books and articles on Bayesian data analysis. Further, there is a growing number of powerful software packages that can be used for Bayesian data analysis.\n",
    "\n",
    "### Some introductory texts\n",
    "\n",
    "These two books provide a broad and readable introduction to Bayesian data analysis... well, sort of. Both books contain extensive examples using R and specialized Bayes packages.\n",
    "\n",
    "<img src=\"img/StatisticalRethinking.jpg\" alt=\"Drawing\" style=\"width:200px; height:275px\"/>\n",
    "\n",
    "<img src=\"img/DoingBaysianDataAnalysis.jpg\" alt=\"Drawing\" style=\"width:200px; height:275px\"/>\n",
    "\n",
    "\n",
    "### Modeling reference\n",
    "\n",
    "This book contains a comprehensive treatment of applying Bayesian models. The level of treatments in intermediate. The examples are from the social sciences, but the methods can be applied more widely. The examples use R and specialized Bayes packages. \n",
    "\n",
    "<img src=\"img/BayesRegression.jpg\" alt=\"Drawing\" style=\"width:200px; height:275px\"/>\n",
    "\n",
    "### Theory \n",
    "\n",
    "This book contains a comprehensive overview of the modern theory of Bayesian models. The book is at an advanced level. Only theory is addressed, which only very limited R code examples.  \n",
    "\n",
    "<img src=\"img/BaysianDataAnalysis.jpg\" alt=\"Drawing\" style=\"width:200px; height:275px\"/>\n",
    "\n",
    "### Software\n",
    "\n",
    "Most Bayes software packages use efficient Markov chain Monte Carlo (MCMC) methods. The most widely used of these is [Stan](https://mc-stan.org/), named for mathematician Stanislaw Ulam. Stan also includes variational approximation methods. A powerful and somewhat more user friendly Python package is [PyMC3](https://docs.pymc.io/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem and Conditional Probability\n",
    "\n",
    "As you might guess from the name, Bayesian methods are built upon **Bayes' Theorem** or **Bayes' Rule**, a fundamental relationship for **conditional probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conditional probability\n",
    "\n",
    "Let's briefly review **conditional probability**. Conditional probability is the probability that event A occurs given that event B has occurred. Consider the example of events in a space S with subspaces A and B, shown below. \n",
    "\n",
    "<img src=\"img/Prob1.png\" alt=\"Drawing\" style=\"width:300px; height:200px\"/>\n",
    "\n",
    "We can write a conditional probability relationship between the subsets as follows. We write the conditional probability of A given B:\n",
    "\n",
    "$$P(A|B)$$\n",
    "\n",
    "Let's try to find the relationship between conditional probability and the intersection between the sets, $P(A \\cap B)$. To find this probability notice that it is the product of two probabilities:  \n",
    "1. $P(B)$ since B must be true to be in this intersection. \n",
    "2. $P(A|B)$ since A must also occur when B is occurring.\n",
    "\n",
    "We can now write:\n",
    "\n",
    "$$P(A \\cap B) = P(A|B) P(B)$$\n",
    "\n",
    "Rearranging terms we get the following for our example: \n",
    "\n",
    "\\begin{align}\n",
    "P(A|B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "& = \\frac{\\frac{2}{10}}{\\frac{4}{10}} = \\frac{2}{4} = \\frac{1}{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "As has already been stated, Bayes' Theorem is fundamental to Bayesian data analysis. To get started, let's go through a simple derivation of Bayes, theorem. From the previous section we have:\n",
    "\n",
    "$$P(A|B) P(B) = P(A \\cap B)$$\n",
    "\n",
    "Using the same approach, we can also write: \n",
    "\n",
    "$$P(B|A) P(A) = P(A \\cap B)$$\n",
    "\n",
    "Eliminating $P(A \\cap B):$\n",
    "\n",
    "$$ P(B)P(A|B) = P(A)P(B|A)$$\n",
    "\n",
    "Or, \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Which is Bayes' Theorem!\n",
    "\n",
    "But, how can you interpret Bayes' Theorem in a way that is useful for data analysis? \n",
    "\n",
    "$$Posterior\\ Distribution = \\frac{Likelihood \\bullet Prior\\ Distribution}{Distribution\\ of\\ Data} $$\n",
    "\n",
    "In general, we are interested in estimating model parameters. In this case, we can think of Bayes' theorem as follows:   \n",
    "\n",
    "$$ùëÉ(ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë†‚îÇùëëùëéùë°ùëé) = \\frac{ùëÉ(ùëëùëéùë°ùëé|ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë†)\\ ùëÉ(ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë†)}{P(data)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example, probabilities of eye and hair color\n",
    "\n",
    "A sample population has the following probabilities of eye and hair color combinations. Execute thhe code to see the chart of conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair = pandas.DataFrame({\n",
    "    'black': [0.11, 0.03, 0.03, 0.01], \n",
    "    'brunette': [0.2, 0.14, 0.09, 0.05],\n",
    "    'red': [0.04, 0.03, 0.02, 0.02],\n",
    "    'blond': [0.01, 0.16, 0.02, 0.03],\n",
    "}, index=['brown', 'blue', 'hazel', 'green'])\n",
    "\n",
    "eye_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: here we're using a string index for eye color rather than a numeric zero-based index. So to access a given (eye, hair) color value, index the dataframe like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair.loc['hazel', 'red']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the table above are the **conditional probabilities**. Note that in the case: \n",
    "\n",
    "$$P(hair|eye) = P(eye|hair)$$ \n",
    "\n",
    "Given these conditional probabilities, it is easy to compute the marginal probabilities by summing the probabilities in the rows and columns. The **Marginal probability** is the probability along one variable (one margin) of the distribution. Computing the marginal distribution is sometimes referred to as **summing out** the conditioning variable. For example, one can compute $P(Red)$ or $P(Green)$. by summing out all the conditioning variables. Like all probability distributions, the probabilities of a marginal distribution must sum to 1.0. \n",
    "\n",
    "The code in the cell below computes the marginal probabilities of eye color and appends the results to the data frame. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the marginal distribution of each eye color\n",
    "eye_hair['marginal_eye'] = eye_hair.sum(axis=1)\n",
    "eye_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code in the cell below to compute the marginal distribution of hair color and append it to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair.loc['marginal_hair'] = eye_hair.sum(axis=0)\n",
    "eye_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What inferences can you make from these marginal distributions? What is the most common eye color? What is the least common hair color?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Use Bayes Theorm to compute the probability of each eye color given that the subject has blue eyes; $P(Hair\\ Color|Blue\\ Eyes)$. Hint, this is a bit tricky since $P(hair\\ color) = 1$ across all colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bayes Theorem\n",
    "\n",
    "We need a formulation of Bayes Theorem which is tractable for computational problems. Specifically, we don't want to be stuck summing all of the possibilities to compute the denominator, $P(B)$. In fact, in many cases, computing this denominator can be intractable.  \n",
    "\n",
    "We can start by examining some fun facts about conditional probabilities. \n",
    "\n",
    "$$\n",
    "ùëÉ(ùêµ \\cap A) = ùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥) \\\\\n",
    "And \\\\\n",
    "ùëÉ(ùêµ)=ùëÉ(ùêµ \\cap ùê¥)+ùëÉ(ùêµ \\cap \\bar{ùê¥}) \\\\\n",
    "Then \\\\\n",
    "ùëÉ(ùêµ)=ùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)+ùëÉ(ùêµ‚îÇ \\bar{ùê¥})ùëÉ(\\bar{ùê¥}) \\\\\n",
    "where \\\\\n",
    "\\bar{A} = Not\\ A\n",
    "$$\n",
    "\n",
    "We can now rewrite Bayes Theorm:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{ùëÉ(ùêµ‚îÇùê¥)ùëÉ(ùê¥)+ùëÉ(ùêµ‚îÇ \\bar{ùê¥})ùëÉ(\\bar{ùê¥})} \\\\ $$\n",
    "\n",
    "In summary, to compute the denominator we need to sum all the cases in the subset A and all the cases not in the subset A. This is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
    "\n",
    "$$ùëÉ(ùê¥‚îÇùêµ)=ùëò‚àôùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)$$\n",
    "\n",
    "Ignoring the normalization constant $k$, we get:\n",
    "\n",
    "$$ùëÉ(ùê¥‚îÇùêµ) \\propto ùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)$$\n",
    "\n",
    "### Applying the simplified relationship Bayes Theorem\n",
    "\n",
    "How to we interpret the relationship shown above? We can do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "ùëÉ( ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë† ‚îÇ ùëëùëéùë°ùëé ) \\propto ùëÉ( ùëëùëéùë°ùëé | ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë† )ùëÉ( ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë† ) $$\n",
    "\n",
    "These relationships can apply to the observed data distributions. Most typically, to parameters in a model, including partial slopes, intercept, error distributions, lasso constant, etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bayes models\n",
    "\n",
    "The goal of a Bayesian model is to find the posterior distribution of parameters. The general steps are as follows:\n",
    "\n",
    "1. Identify data relevant to the research question. Unlike for frequentist models, data need not be collected in advance.   \n",
    "2. Define a descriptive model for the data. For example, a linear model formula might be used for some problems.\n",
    "3. Specify a prior distribution of the model parameters. For example, you might believe that the parameters of the linear model should be Normally distributed as $N(\\theta,\\sigma^2)$.\n",
    "4. Use the Bayesian inference formula (above) to compute posterior distribution of the model parameters. IF there is no data as yet, the posterior distribution is the same as the prior distribution. \n",
    "5. Update if more data is observed. This is key! The posterior of a Bayesian model naturally updates as more data is added, a form of learning.\n",
    "6. Optionally, simulate data values from realizations of the posterior distribution of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How can you choose a prior?\n",
    "\n",
    "The choice of the prior is a serious problem when performing Bayesian analysis. In fact, the need to choose a prior has often been cited as a reason why Bayesian models are impractical. A general consideration is that a prior must be convincing to a **skeptical audience**. \n",
    "\n",
    "There are a number of ways one can come up with a good prior distribution. Some possible approaches include:\n",
    "\n",
    "- Using prior empirical information about the problem. This might include come from \n",
    "- Apply domain knowledge to determine a reasonable distribution. For example, viable range of parameter values could be computed from physical principles. \n",
    "- If there is poor prior knowledge for the problem use less informative prior. One possibility is a Uniform distribution. But **watch out**, a uniform prior is informative, since you must set the limits on range of values! There can be other options for uninformative distributions, such as the Jefferys' prior. \n",
    " \n",
    "One analytically and computationally simple choice for a prior distribution family is a **conjugate prior**. When a likelihood is multiplied by a conjugate prior the distribution of the posterior is the same as the likelihood. Most named distributions have conjugates. A few commonly used examples are shown in the table below:\n",
    "\n",
    "Likelihood | Conjugate\n",
    "---|---\n",
    "Binomial|Beta\n",
    "Bernoulli|Beta\n",
    "Poisson|Gamma\n",
    "Categorical|Dirichlet\n",
    "Normal| Normal, Inverse Gamma\n",
    "\n",
    "However, there are many practical cases where a conjugate prior is not used. With modern computational methods, a conjugate distribution is not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example\n",
    "\n",
    "With a bit of theory in mind, let's pull things together with an example. Let's say we are interested in analyzing distracted drivers. We randomly sample the behavior of 10 drivers at an intersection and determine if they exhibit distracted driving or not. The data are binomially distributed, a driver is distracted or not. In the example we will:\n",
    "\n",
    "1. Select the Binomial distribution for the likelihood.\n",
    "2. Choose a Uninformative distribution as the prior.   \n",
    "3. Using the data sample, compute the likelihood.\n",
    "4. Compute the posterior distribution of distracted driving. \n",
    "5. Try another prior distribution and repeat step 4. Specifically use the conjugate prior, the Beta distribution with parameters $\\alpha$ and $\\beta$.\n",
    "5. Add more data to our data set to updated the posterior distribution.\n",
    "\n",
    "The likelihood of the data and the posterior distribution are binomially distributed. The Binomial distribution has one parameter we need to estimate, $p$, the probability. We can write this formally for $k$ successes in $N$ trials:\n",
    "\n",
    "$$ P(A) = \\binom{N}{k} \\cdot p^k(1-p)^{N-k}$$\n",
    "\n",
    "The code in the cells below creates a simple data set of distracted and not-distracted drivers and computes some simple summary statistics. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = ['yes','no','yes','no','no','yes','no','no','no','yes']\n",
    "distracted = [1 if x is 'yes' else 0 for x in drivers]\n",
    "distracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(distracted)  # sample size\n",
    "n_distracted = sum(distracted)  # number of distracted drivers\n",
    "n_not = N - n_distracted # number not distracted\n",
    "print('Distracted drivers = %d \\nAttentive drivers = %d'\n",
    "    '\\nProbability of distracted driving = %.1f' \n",
    "      % (n_distracted, n_not, n_distracted / (n_distracted + n_not)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a prior distribution for our one model parameter $p$, $P(p)$. We don't know a lot about these drivers at this point, so we will start with a Uniform distribution. To be completely uninformative, we will set the limits of our prior for $p$ as $\\{0,1\\}$. \n",
    "\n",
    "The code in the cell below computes and plots the uniform prior distribution. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = numpy.linspace(.01, .99, num=N)\n",
    "pp = [1./N] * N\n",
    "plt.plot(p, pp, linewidth=2, color='blue')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Probability density')\n",
    "_=plt.title('Uniform prior distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compute the likelihood. The likelihood is the probability of the data given the parameter, $P(X|p)$, using the Binomial distribution. The code in the cell below computes and plots the Binomial likelihood for the distracted driver data. This calculation is performed for each value of $p$ we are sampling. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(p, data):\n",
    "    k = sum(data)\n",
    "    N = len(data)\n",
    "    # Compute Binomial likelihood\n",
    "    l = scipy.special.comb(N, k) * p**k * (1-p)**(N-k)\n",
    "    # Normalize the likelihood to sum to unity\n",
    "    return l/sum(l)\n",
    "\n",
    "l = likelihood(p, distracted)\n",
    "plt.plot(p, l)\n",
    "plt.title('Likelihood function')\n",
    "plt.xlabel('Parameter')\n",
    "_=plt.ylabel('Likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about the likelihood:\n",
    "1. The maximum is at $p=0.4$, the actual parameter value.\n",
    "2. The likelihood of high and low parameter values is nearly 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a prior and a likelihood we are in a position to compute the posterior distribution of the parameter $p$, $P(p|X)$. The code in the cell below computes and plots the posterior, given the prior and likelihood.\n",
    "\n",
    "> **Warning!** The computational methods used in this notebook are simplified for the purpose of illustration. For real-world problems, computationally efficient code must be used!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(prior, like):\n",
    "    post = prior * like # compute the product of the probabilities\n",
    "    return post / sum(post) # normalize the distribution to sum to unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_post(prior, like, post, x):\n",
    "    maxy = max(max(prior), max(like), max(post))\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(x, like, label='likelihood', linewidth=12, color='black', alpha=.2)\n",
    "    plt.plot(x, prior, label='prior')\n",
    "    plt.plot(x, post, label='posterior', color='green')\n",
    "    plt.ylim(0, maxy)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.title('Density of prior, likelihood and posterior')\n",
    "    plt.xlabel('Parameter value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "post = posterior(pp, l)\n",
    "plot_post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum of the prior density = %.3f' % max(pp))\n",
    "print('Maximum likelihood = %.3f' % float(float(numpy.argmax(l))/100.0 + 0.01))\n",
    "print('MAP = %.3f' % float(float(numpy.argmax(post))/100.0 + 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with Uniform prior distribution, the posterior is just the likelihood. This is an important observation. The key point is that the frequentist probabilities are identical to the Bayesian posterior distribution given a Uniform prior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another prior\n",
    "\n",
    "Let's try another prior distribution. We will chose the **conjugate prior** of the Binomial distribution which is the Beta distribution. Formally, we can write the Beta distribution:\n",
    "\n",
    "$$Beta(p |a, b) = \\kappa x^{a-1}(1 - x)^{b-1} \\\\\n",
    "where,\\ \\kappa = normalization\\ constant$$\n",
    "\n",
    "The Beta distribution is define on the interval $0 \\le Beta(p|a,b) \\le 1$. The Beta distribution has two parameters, $a$ and $b$, which determine the shape. To get a feel for the Beta distribution, execute the code in the cell below which computes 25 examples on a 5x5 grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "alpha = [.5, 1, 2, 3, 4]\n",
    "beta = alpha[:]\n",
    "x = numpy.linspace(.001, .999, num=100)\n",
    "\n",
    "for i, (a, b) in enumerate(itertools.product(alpha, beta)):\n",
    "    plt.subplot(len(alpha), len(beta), i+1)\n",
    "    plt.plot(x, scipy.stats.beta.pdf(x, a, b))\n",
    "    plt.title('(a,b) = (%d,%d)' % (a,b))\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plots above, that the Beta distribution can take on quite a range of shapes, depending on the parameters. Generally if $a \\gt b$ the distribution skews to the rights, if $a \\lt b$ to the left, and symmetric if $ a = b$. Notice also that $Beta(1,1)$ is the Uniform distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still do not know a lot about the behavior of drivers, so we will pick a rather vague or broad Beta distribution as our prior. The code in the cell below uses a symmetric prior with $a = 2$ and $b = 2$. This prior is fairly vague and assumes that the maximum value of $p=0.5$. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_prior(x, a, b):\n",
    "    l = scipy.stats.beta.pdf(p, a, b)  # compute likelihood\n",
    "    return l / l.sum()  # normalize and return\n",
    "\n",
    "pp = beta_prior(p, 2, 2)\n",
    "post = posterior(pp, l)\n",
    "plot_post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum of the prior density = %.3f' % float(float(numpy.argmax(pp))/100.0 + 0.01))\n",
    "print('Maximum likelihood = %.3f' % float(float(numpy.argmax(l))/100.0 + 0.06))\n",
    "print('MAP = %.3f' % float(float(numpy.argmax(post))/100.0 + 0.06))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mode of the posterior is close to the mode of the likelihood, but has shifted toward the mode of the prior. We call this tendency of Bayesian posteriors to shift toward the prior the **shrinkage property**. The tendency of the maximum likelihood point of the posterior is said to shrink toward the maximum likelihood point of the prior. \n",
    "\n",
    "We can now see that the posterior probability of distracted driving has a rather wide spread. How can we get a more definitive understanding of the probability of distracted driving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data to the Bayesian model\n",
    "\n",
    "Let's say that we observe some more drivers to gather more data on distracted driving. Additional data will narrow the spread of the posterior distribution. Further, as data is added to a Bayesian model, the posterior moves toward the likelihood. This property has several important implications:\n",
    "\n",
    "- The prior matters less as more data is added to a Bayesian model.\n",
    "- Adding data reduces shrinkage.\n",
    "- The inferences from Bayesian and frequentist models tend to converge as data set size grows and the posterior approaches the likelihood.\n",
    "\n",
    "The use of a prior can mean that Bayesian methods provide useful inferences with minimal data. **But, be careful!** For large scale problems with large numbers of parameters you may need enormous data sets to see the convergence in behavior. \n",
    "\n",
    "The code in the cell below adds another 10 observations to our data set. Execute this code and examine the results. How do the likelihood and posterior distributions compare with the case with only 10 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_drivers = ['no','yes','no','no','no',\n",
    "          'yes','no','yes','no','no']  # Some new data\n",
    "new_distracted = [1 if x is 'yes' else 0 for x in new_drivers]\n",
    "\n",
    "l = likelihood(p, distracted + new_distracted)\n",
    "post = posterior(pp, l)\n",
    "plot_post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credible Intervals\n",
    "\n",
    "A **credible interval** is an interval on the Bayesian posterior distribution. The credible interval is sometime called the highest density interval (HDI), or highest posterior density interval (HPI). As an example, the 90% credible interval encompasses the 90% of the posterior distribution with the highest probability density.  \n",
    "\n",
    "The credible interval is the Bayesian analog of the frequentist confidence interval. However, these two measures are conceptually different. The confidence interval is chosen on the distribution of a test statistic, whereas the credible interval is computed on the posterior distribution of the parameter. For symmetric distributions the credible interval can be numerically the same as the confidence interval. However, in the general case, these two quantities can be quite different.  \n",
    "\n",
    "The code in the cell below, plots the posterior distribution of the parameter of the binomial distribution parameter  pp . An approximation of the 95% credible interval, or HDI, is also computed and displayed. Execute this code and examine the result. \n",
    "\n",
    "\n",
    "> **Warning!** This code assumes a symmetric prior distribution, so will not work in the general case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "lower_q, upper_q = [.025, .975]\n",
    "\n",
    "def plot_ci(p, post, num_samples, lower_q, upper_q):\n",
    "    ## This function computes a credible interval using an assumption\n",
    "    ## of symetry in the bulk of the distribution to keep the \n",
    "    ## calculation simple. \n",
    "    ## Compute a large sample by resampling with replacement\n",
    "    samples = numpy.random.choice(p, size=num_samples, replace=True, p=post)\n",
    "    ci = scipy.percentile(samples, [lower_q*100, upper_q*100]) # compute the quantiles\n",
    "    \n",
    "    interval = upper_q - lower_q\n",
    "    plt.title('Posterior density with %.3f credible interval' % interval)\n",
    "    plt.plot(p, post, color='blue')\n",
    "    plt.xlabel('Parameter value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.axvline(x=ci[0], color='red')\n",
    "    plt.axvline(x=ci[1], color='red')\n",
    "\n",
    "    print('The %.3f credible interval is %.3f to %.3f' \n",
    "          % (interval, ci[0], ci[1]))\n",
    "    \n",
    "plot_ci(p, post, num_samples, lower_q, upper_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating from the  posterior distribution: forecasting\n",
    "\n",
    "So far, we have computed the posterior distribution of the probability parameter $p$. But what about the distribution of distracted drivers? We can compute this distribution by simulating from the posterior distribution of $p$. \n",
    "\n",
    "The code in the cell below simulates and plots the distribution of distracted drivers. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cars = 10\n",
    "num_samples = 10000\n",
    "\n",
    "counts = (10 * numpy.random.choice(p, size=num_samples, replace=True, p=post)).round()\n",
    "plt.hist(counts, bins=int(max(counts)))\n",
    "plt.title('Probability vs. number of distracted drivers in next %d' % num_cars)\n",
    "plt.xlabel('Number of distracted drivers')\n",
    "plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Bayesian models\n",
    "\n",
    "How can we use Bayesian models to compare two distributions? It turns out that we can compare Bayesian models in several ways. In this lesson, we will compute and compare confidence intervals of the posterior distribution of a model parameter. \n",
    "\n",
    "For this example, we will compare the posterior distribution of the heights of sons to the heights of the mothers in the Galton Family dataset. As a first step, we will compute and evaluate Bayesian models for the mean heights using a subset of just 25 observations. \n",
    "\n",
    "The code in the cell below sub-samples the Galton family data and then plots histogram of the heights of sons and mothers. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = pandas.read_csv('GaltonFamilies.csv', index_col=0)\n",
    "families.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "male = families[families.gender == 'male'].sample(n=num_samples)\n",
    "\n",
    "plt.title('Histograms of heights of people')\n",
    "male.childHeight.hist(label='sons', bins=10, alpha=.7)\n",
    "male.mother.hist(label='mothers', bins=10, alpha=.7)\n",
    "plt.xlabel('Height')\n",
    "plt.legend()\n",
    "\n",
    "mean_height = numpy.concatenate([male.mother, male.father, male.childHeight,]).mean()\n",
    "print('Mean of heights: %.1f' % mean_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this analysis, we need to select a prior distribution and compute the likelihood. \n",
    "\n",
    "First, we will address the likelihood. For these data, we will use a Normal likelihood. For a sample $X = {x_1, x_2, \\ldots, x_n}$, we can write the likelihood as:\n",
    "\n",
    "$$\n",
    "P(X | u, \\sigma) = \\bigg(\\frac{1}{2 \\pi \\sigma^2} \\bigg)^{\\frac{n}{2}} exp \\Bigg[ -\\frac{1}{2 \\sigma^2}  \\Bigg( \\sum_{i = 1}^n (x_i - \\bar{x})^2 + n(\\bar{x} - \\mu)^2 \\Bigg) \\Bigg] \n",
    "$$\n",
    "Ignoring constants and normalization:\n",
    "$$\n",
    "P(X | u, \\sigma) \\propto exp \\bigg( -\\frac{n(\\bar{x} - \\mu)^2}{2 \\sigma^2} \\bigg) \n",
    "$$\n",
    "\n",
    "To simplify the computations here, we will only estimate the posterior distribution of the mean. We will use a fixed empirical estimate of the standard deviation. A more complete analysis will also estimate the posterior distribution of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "p = numpy.linspace(60, 75, num=N)\n",
    "\n",
    "pp = scipy.stats.norm.pdf(p, loc=male.childHeight.mean(), scale=5)\n",
    "pp = pp / pp.sum() # normalize\n",
    "\n",
    "def comp_like(p, x): \n",
    "    variance = numpy.std(x)**2 # sigmasqr\n",
    "    x_mean = numpy.asarray(x).mean()  # xbar\n",
    "    print('Mean = %.3f, Standard deviation = %.3f' % (x_mean, numpy.std(x)))\n",
    "    n = len(x)\n",
    "    l = numpy.exp(-n * numpy.square(x_mean - p) / (2 * variance))\n",
    "    return l / l.sum()\n",
    "\n",
    "like_son = comp_like(p, male.childHeight)\n",
    "post_son = posterior(pp, like_son)\n",
    "\n",
    "plt.plot(p, pp, label='prior')\n",
    "plt.plot(p, like_son, label='likelihood', alpha=.3, linewidth=10)\n",
    "plt.plot(p, post_son, label='posterior')\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes the posterior distribution of the heights of the mothers. Run this code and examine the results. How do these results differ from the results for the heights of the sons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mom = scipy.stats.norm.pdf(p, loc=male.mother.mean(), scale=5)\n",
    "pp_mom = pp_mom / pp_mom.sum() # normalize\n",
    "\n",
    "like_mom = comp_like(p, male.mother)\n",
    "post_mom = posterior(pp_mom, like_mom)\n",
    "\n",
    "plt.plot(p, pp_mom, label='prior')\n",
    "plt.plot(p, like_mom, label='likelihood', alpha=.3, linewidth=10)\n",
    "plt.plot(p, post_mom, label='posterior')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the posterior distributions of the mean heights of the sons to the distribution of the mean heights of the mothers, we compute and compare the confidence intervals. \n",
    "\n",
    "Run the code in the cell below which computes and plots the confidence intervals for the mean heights of the sons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "\n",
    "plot_ci(p, post_son, num_samples, lower_q=.025, upper_q=.975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute and plot the posterior distribution and CIs of the mean of the heights of the mothers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ci(p, post_mom, num_samples, lower_q=.025, upper_q=.975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the CIs for these posterior distributions. Are the distributions of the mean heights of sons and mothers significantly different?  \n",
    "\n",
    "***\n",
    "**Your turn:** Try the variations on the foregoing Bayesian analysis.\n",
    "\n",
    "1. The foregoing Bayesian analysis was performed with just 25 data points. Rerun this analysis with with 250 data points. How do the posterior distributions computed with the 250 data points compare to those computed with 25 data points? \n",
    "2. Perform the same analysis, except comparing the distributions of heights of sons and heights of fathers. \n",
    "\n",
    "**Important!:** Use another variable name for the likelihood and posterior for this exercise. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation from the posterior distribution\n",
    "\n",
    "Once we have a posterior distribution for parameters we can simulate from this distribution. The simulation consists of taking a number of random draws from the posterior parameter distribution and computing the posterior distribution of the data values. \n",
    "\n",
    "There are a number of reasons why you might want to simulate from the posterior distribution of data values.\n",
    "\n",
    "- Test the model against the data.\n",
    "- Compute forecasts of the dependent (label) variable from the model.\n",
    "\n",
    "The code in the cell below computes the posterior distribution of the heights of sons. This is done by computing the distribution of height based on realizations of the parameter (the mean) from the posterior distribution. The density distribution of the simulated heights is plotted along with the histogram of the original data. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_height(n, sigma, p, post):\n",
    "    # create the probability-weighted random sample of values of the mean height\n",
    "    mu = numpy.random.choice(p, size=n, replace=True, p=post)\n",
    "    return scipy.stats.norm.rvs(loc=mu, scale=sigma, size=n)\n",
    "\n",
    "def plot_dist(n, post, dat):\n",
    "    seaborn.distplot(post, bins=20)\n",
    "    plt.title('Histogram of data with density of model predictions')\n",
    "    plt.xlabel('Hight in inches')\n",
    "\n",
    "sim_vals = sim_height(10000, sigma=2.616329, p=p, post=post_son)\n",
    "plot_dist(10000, sim_vals, male.childHeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this chart. How well does the density of the posterior value distribution match the histogram of the original data values? Does the density of the posterior value distribtion deviate from Normal? \n",
    "\n",
    "***\n",
    "**Your turn:** Use the model you computed with 250 data values to create a plot like the one above. Examine these results and compare them to the results obtained with the model created with only 25 data values. Which posterior density function appears to represent the data better and why? Does the density deviate from Normal?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you have explored the following concepts:\n",
    "\n",
    "1. Application of Bayes Theorem.\n",
    "2. Computation of marginal distribtuions.\n",
    "3. Selection and computation of prior distributions.\n",
    "4. Selection and computation of likelihoods.\n",
    "5. Computation of posterior distributions.\n",
    "6. Computation and comparison of credible intervals. \n",
    "7. Simulation of data values from posterior distribution of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/BayesDeNeon.jpg\" alt=\"Drawing\" style=\"width:400px; height:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
